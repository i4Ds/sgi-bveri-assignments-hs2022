{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274e9ba6",
   "metadata": {},
   "source": [
    "## FHNW beverI - HS2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39280e4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f10f19697935a18db955753fdc25539f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a66f2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55b502445c9aeab3a8b3aa0e1feae4ad",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Segmentation\n",
    "\n",
    "Diese Übung basiert zum Teil auf: [https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)\n",
    "\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "- Instance Segmentation: Anwenden von Pre-Trained Modellen, Verstehen & Evaluieren der Outputs\n",
    "- Semantic Segmentation: Design, Trainieren und Evaluieren von Modellen\n",
    "- Upsampling: Techniken kennen und anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458ede3-41f5-44c2-b2b6-8234f84534ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c8491c27913fd45d4acfcc92dc1fbc",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599c65d-4d0d-4930-b4a6-ca9ad201093a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb1a1059b53d6d2fe6e9c69b636321b4",
     "grade": false,
     "grade_id": "parameter",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Parameter\n",
    "\n",
    "Definieren Sie den Pfad zu den Daten und extrahieren Sie die Daten.\n",
    "\n",
    "Für Colab-Nutzer: Führen Sie folgenden Code aus um die Daten zu lesen:\n",
    "\n",
    "\"\"\"\n",
    "%%bash\n",
    "\n",
    "!mkdir /content/data\n",
    "\n",
    "!wget https://github.com/i4Ds/sgi-bveri-assignments-hs2022/tree/main/assignments/04_segmentation/data.tar.gz -P /content/data/ \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293eec1-0f64-4e63-9a1b-6b6ca61329ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3916c6-f40f-477a-b059-dd29568075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ROOT_PATH=$(pwd)\n",
    "tar -xvzf ${ROOT_PATH}/data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d8d49-4072-4e21-aded-9b0889cda439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "371cc3de758e3ccdcb1ea83caa805116",
     "grade": false,
     "grade_id": "instance",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Instance Segmentation mit _Mask R-CNN_\n",
    "\n",
    "Wenden Sie ein vortrainiertes Modell der _Mask R-CNN_ Familie von [torchvision](https://pytorch.org/vision/stable/models.html#instance-segmentation) an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ada36-4d57-4c34-99c5-92d9f1f46d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "img_dogs = Image.open(DATA_PATH.joinpath(\"dog.jpg\"))\n",
    "img_ducks = Image.open(DATA_PATH.joinpath(\"ducks.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d609668-c17f-4021-bd3f-107357250e15",
   "metadata": {},
   "source": [
    "Initialisieren Sie das Modell und führen Sie einen forward pass aus über die beiden Beispiel-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031a948-f019-4145-a8da-63bc039e93d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf2848ddec5e7495a1bab2dd472007b",
     "grade": true,
     "grade_id": "cell-be98073989cf4c41",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8813b-19f1-438c-aaf1-fd01a750c3e6",
   "metadata": {},
   "source": [
    "Inspizieren Sie den Output von _Mask R-CNN_. Wieviele Objekte hat das Modell gefunden? Was wird alles ausgegeben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2e497-ca62-4f2d-a6dd-9157bbd219e7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f93e184bc906a5f8dd17693cbac8032",
     "grade": true,
     "grade_id": "cell-1fbbeeed9c0a6bdd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1f204-66db-49c3-bac3-86201e86e0e2",
   "metadata": {},
   "source": [
    "Zeichnen Sie die gefundenen Objekte auf das Bild. Probieren Sie verschiedene Werte für `score_threshold` und `proba_threshold`. Was bedeuten die Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c28b30-5643-46c6-9136-43c0315845cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torchshow as ts\n",
    "\n",
    "score_threshold = .75\n",
    "proba_threshold = 0.5\n",
    "\n",
    "boolean_masks = [\n",
    "    out['masks'][out['scores'] > score_threshold] > proba_threshold\n",
    "    for out in output\n",
    "]\n",
    "\n",
    "images_with_masks = [\n",
    "    draw_segmentation_masks(torch.tensor(np.array(img)).permute(2, 0, 1), mask.squeeze(1))\n",
    "    for img, mask in zip(images_list, boolean_masks)\n",
    "]\n",
    "ts.show(images_with_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d3a6d-4dc7-4b4e-abe3-ab515fa2ce01",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "Hier schauen wir verschiedene Upsampling Techniken an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052fb05-1493-4507-9c98-355da913483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "to_upsample = torch.tensor([[1, 2], [3, 4]]).unsqueeze(0).to(torch.float)\n",
    "to_upsample_2 = torch.concat([to_upsample, to_upsample], dim=2)\n",
    "to_upsample_2 = torch.concat([to_upsample_2, to_upsample_2], dim=1)\n",
    "\n",
    "    \n",
    "def display_arrays(arrays: List[np.ndarray], titles: List[str]):\n",
    "    \"\"\" Display Arrays \"\"\"\n",
    "    num_arrays = len(arrays)\n",
    "    kwargs = {'annot': True, 'cbar': False, 'vmin': 0, 'vmax': 10, 'xticklabels': False, 'yticklabels': False}\n",
    "    fig, ax = plt.subplots(figsize=(3 * num_arrays, 3), ncols=num_arrays)\n",
    "    \n",
    "    # handle single and multi-array plots\n",
    "    if num_arrays > 1:\n",
    "        axes = ax.flatten()\n",
    "    else:\n",
    "        axes = [ax]\n",
    "    \n",
    "    for i, (array, title) in enumerate(zip(arrays, titles)):\n",
    "        sns.heatmap(array, **kwargs, ax=axes[i]).set(\n",
    "            title=f\"{title} - Shape {array.shape}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "display_arrays([np.array(to_upsample[0, :, :])], [\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a743e-9145-4cc8-b3f7-775913163d79",
   "metadata": {},
   "source": [
    "### Unpooling\n",
    "\n",
    "Testen Sie Max-Pooling und Max-Unpooling mit Switch. Probieren Sie verschiedene Parameter aus und schauen Sie sich das Ergebnis an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790330a-c843-414e-925d-618752237788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "to_upsample_3 = torch.clone(to_upsample_2)\n",
    "to_upsample_3[0, 0, 0] = 16\n",
    "\n",
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "output, indices = pool(to_upsample_3)\n",
    "unpooled = unpool(output, indices)\n",
    "\n",
    "display_arrays(arrays=[\n",
    "    np.array(to_upsample_3[0, :, :]),\n",
    "    np.array(output[0, :, :]),\n",
    "    np.array(unpooled[0, :, :])],\n",
    "    titles=[\"input\", \"pooled\", \"unpooled\"]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2145386-bcff-4841-8e88-ee0db8c113e7",
   "metadata": {},
   "source": [
    "### Transpose Convolution\n",
    "\n",
    "Testen Sie verschiedene Parameter für die _Transposed Convolution_. Erstellen Sie 2 weitere Varianten und visualisieren Sie die Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97d909-5acb-4839-9b30-64d67c938c80",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d14dd895eb7c9e8b7ac3a89cd863039",
     "grade": true,
     "grade_id": "trans_conv",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "weight = torch.tensor([[1, 2, 3], [0, 1, 2], [0, 1, 2]]).unsqueeze(0).unsqueeze(0).to(torch.float)\n",
    "weight.shape\n",
    "\n",
    "out = F.conv_transpose2d(input=to_upsample_2, weight=weight, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "arrays_to_plot = [np.array(x) for x in [\n",
    "    to_upsample_2[0, : :], weight[0, 0, : :], out[0, : :], out2[0, : :], out3[0, : :]]]\n",
    "\n",
    "display_arrays(\n",
    "    arrays=arrays_to_plot,\n",
    "    titles=[\"Input\", \"Filter\", \"Output\", \"Output2\", \"Output3\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0544f-330b-430e-9075-ef4e8209edcb",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "Hier werden Sie ein _Fully-Convolutional Network_ trainieren für _Semantic Segmentation_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b1ca8-31c6-4120-98de-20f586f6a227",
   "metadata": {},
   "source": [
    "### Stanford Background Dataset\n",
    "\n",
    "Wir schauen uns das [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html) an. Dort gibt es verschiedene Szenerien mit _semantic segmentation_ Annotationen.\n",
    "\n",
    "Lesen Sie als erstes ein Beispiel-Bild ein und stellen Sie es dar:\n",
    "\n",
    "DATA_PATH.joinpath(\"iccv09Data/images/0000047.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef9d0e-fa1d-4f07-bcf4-bea851f6127e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b554d5642e2aa8bbdb9f2243e0c949",
     "grade": true,
     "grade_id": "read_image",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfb291-5cf4-4a8e-96e9-1eb9b2ce63f1",
   "metadata": {},
   "source": [
    "Wir definieren nun ein `torch.utils.data.Dataset` um ein Bild und dessen _Segmentation Map_ auszugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135dc926-cb8d-4839-8187-02b1ef3e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StanfordBackgroundDataset(Dataset):\n",
    "    \"\"\" StanfordBackground Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path: Path, transform=None, transform_labels=None):\n",
    "        self.root_path = root_path\n",
    "        self.transform = transform\n",
    "        self.transform_labels = transform_labels\n",
    "        self.image_paths = list(root_path.joinpath(\"images\").glob(\"*.jpg\"))\n",
    "        self.classes = [\"sky\", \"tree\", \"road\", \"grass\", \"water\", \"building\", \"mountain\", \"foreground object\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns: Image (CxHxW), Label Mask (KxHxW), Label Image (1xHxW)\"\"\"\n",
    "        \n",
    "        path = self.image_paths[idx]\n",
    "        image = np.array(Image.open(path))\n",
    "        \n",
    "        label_path = self.root_path.joinpath(f\"labels/{path.stem}.regions.txt\") \n",
    "        labels = self._parse_regions(label_path)\n",
    "        \n",
    "        mask = torch.zeros(len(self.classes), labels.shape[0], labels.shape[1])\n",
    "        labels = torch.tensor(labels).unsqueeze(0)\n",
    "        labels = torch.clip(labels, 0, len(self.classes)-1)\n",
    "        label_masks = mask.scatter_(0, labels, 1.)\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.transform_labels:\n",
    "            label_masks = self.transform_labels(label_masks)\n",
    "            labels = self.transform_labels(labels)\n",
    "            \n",
    "        return image, label_masks, labels\n",
    "    \n",
    "    def _parse_regions(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = list()\n",
    "            for x in f.readlines():\n",
    "                line = [int(num.strip('\\n')) for num in x.split(' ')]\n",
    "                lines.append(line)\n",
    "            label_image = np.stack(lines)\n",
    "        return label_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19abebf-00c0-4b67-a89c-3a86d66dd85d",
   "metadata": {},
   "source": [
    "Visualisieren Sie einige Inputs und Labels mit `torchshow.show`, die Sie von einem Objekt der Klasse `StanfordBackgroundDataset` beziehen. Überprüfen Sie ob die Daten korrekt aussehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266cad3-8527-4276-934c-017956fa4a48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68aff8eb01ebe9060b7153a3158f46a2",
     "grade": true,
     "grade_id": "dataset",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_path = DATA_PATH.joinpath(\"iccv09Data\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0826ee-92b3-4c2a-8a33-0a28f99b2fb8",
   "metadata": {},
   "source": [
    "### Fully-Convolutional Network\n",
    "\n",
    "Implementieren Sie ein FCN mit einer Encoder-Decoder Architektur. Ergänzen Sie die Klassen entsprechend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217a141-4328-4884-82ee-3c5be0d432ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4151753a83c0addd5ecb4d258069853e",
     "grade": true,
     "grade_id": "fcn",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\" Encoder-Decoder \"\"\"\n",
    "    def __init__(self, encoder, decoder, num_initial_channels, num_input_channels, num_output_channels):\n",
    "        super().__init__()\n",
    "        self.input = nn.Conv2d(3, num_initial_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output = nn.Conv2d(num_input_channels,  num_output_channels, kernel_size=(1, 1), stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        # Transposed Convolution Layer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for i, (in_channels, out_channels) in enumerate(zip(num_channels, num_channels[1:])):\n",
    "            self.layers_.append(EncoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for i, (in_channels, out_channels) in enumerate(zip(num_channels, num_channels[1:])):\n",
    "            self.layers_.append(DecoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9aa9e-03a1-4c1e-8b78-baf1cef9e9c8",
   "metadata": {},
   "source": [
    "Überprüfen Sie die Architektur. Z.B. das die Output-Shape korrekt ist. Wir möchten pro Klasse eine eigene Maske erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c377-77ac-417b-b3b6-e70df02b8698",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d554a02aff71f0d3f2d02cbe4202836d",
     "grade": true,
     "grade_id": "model",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((200, 300)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])\n",
    "\n",
    "transf_labels = transforms.Compose([\n",
    "    transforms.CenterCrop((200, 300))\n",
    "])\n",
    "\n",
    "ds = StanfordBackgroundDataset(root_path, transform=transf)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192bf32-1ba3-4a0d-945c-ac22032fb1e5",
   "metadata": {},
   "source": [
    "### Model-Training und Metriken\n",
    "\n",
    "Nun werden wir das Modell trainieren und den Fortschritt monitoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fdccd-c69f-438a-ab6a-503189e638db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((200, 300)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "])\n",
    "\n",
    "transf_labels = transforms.Compose([\n",
    "    transforms.CenterCrop((200, 300))\n",
    "])\n",
    "\n",
    "ds = StanfordBackgroundDataset(root_path, transform=transf, transform_labels=transf_labels)\n",
    "\n",
    "ds_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6363f-ed42-4a47-87f9-33c0de47e1a4",
   "metadata": {},
   "source": [
    "Ergänzen Sie den Trainings-Loop wo nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b36226-e241-4d4e-938d-672ef6615fd6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4207a37fd4530a97dbad53d286898cfd",
     "grade": true,
     "grade_id": "train",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 8\n",
    "\n",
    "# create model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Create Loss-Function and Optimizer\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "pbar = tqdm(total=num_epochs * len(ds_loader))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(ds_loader):\n",
    "        \n",
    "        images, label_masks, label_images = data\n",
    "        \n",
    "        # Forward-Pass\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Optimize\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Calculate Pixel-Accuracy\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_acc += pixel_acc\n",
    "        step += 1\n",
    "        print_every = 10\n",
    "        if (i % print_every) == (print_every - 1):\n",
    "            desc = f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_every:.3f} acc: {running_acc / print_every:.3f}'\n",
    "            _ = pbar.update(print_every)\n",
    "            _ = pbar.set_description(desc)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "pbar.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334187-7e84-499f-9c67-bd763d6ea59e",
   "metadata": {},
   "source": [
    "Visualisieren Sie die Vorhersage auf einem Bild und vergleichen Sie mit der annotierten _Segmentation map_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab730b9c-e4ac-4016-ba19-6c36da815361",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62a4de3445a648e6fea89cd5a13ee2fc",
     "grade": true,
     "grade_id": "pred",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
